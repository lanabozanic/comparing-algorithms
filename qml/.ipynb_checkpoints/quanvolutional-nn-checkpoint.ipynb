{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from https://pennylane.ai/qml/demos/tutorial_quanvolution.html \n",
    "# datasets: https://keras.io/api/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Quanvolutional Neural Networks\n",
    "\n",
    "#### Classical Convolutional NN\n",
    "CNNs are a useful classical machine learning tool with a big application in the world of image processing. The way they work is that they use a *convolutational* layer to process smaller parts of an input image. They repeat this process with every small local subsection of the input image, and using the same kernel to compute them all. The results obtained for each region are usually associated to different channels of a single output pixel. The union of all the output pixels produces a new image-like object, which can be further processed by additional layers. [1]\n",
    "\n",
    "#### Quantum Convolutional NN\n",
    "In a similar fashion, we can extend the idea of using a variational quantum circuits to achieve the same outcome (with a possible quantum speed-up in the near-term). We can encode a region of the input image into our quantum circuit, and then use a unitary operator as our convolutional layer. Afterwards, we measure the quantum circuit (and be post-processed, or used raw). Each expectation value we obtain can then be mapped to a different channel of a single output pixel (similar to classical convolution). We repeat this until we have processed an entire image.\n",
    "\n",
    "The main difference with respect to a classical convolution is that a quantum circuit can generate highly complex kernels whose computation could be, at least in principle, classically intractable. [1]\n",
    "\n",
    "\n",
    "#### Steps for the Quanvolutional Neural Net Algorithm\n",
    "1. a small region of the input image is embedded into a quantum circuit\n",
    "2. define and apply our unitary operator (analgous to the classical convolution layer) \n",
    "3. measure and obtain the expectation values associated with each pixel\n",
    "4. repeat 1-3 until the whole input image is processed\n",
    "5. (optional) the quantum convolution can be followed by further quantum layers or by classical layers.\n",
    "\n",
    "\n",
    "references:\n",
    "\n",
    "[1] https://pennylane.ai/qml/demos/tutorial_quanvolution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image of the Quanvolution Neural Net Process from pennylane:\n",
    "\n",
    "![alt text](images/qcnn.png \"QCNN Circuit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 3 #Number of random layers\n",
    "n_train = 30    # Size of the train dataset\n",
    "n_test = 30     # Size of the test dataset\n",
    "\n",
    "SAVE_PATH = \"quanvolution/\" # Data saving folder\n",
    "PREPROCESS = True           # If False, skip quantum processing and load data from SAVE_PATH\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "# Reduce dataset size\n",
    "train_images = train_images[:n_train]\n",
    "train_labels = train_labels[:n_train]\n",
    "test_images = test_images[:n_test]\n",
    "test_labels = test_labels[:n_test]\n",
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = train_images[..., tf.newaxis]\n",
    "test_images = test_images[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"lightning.qubit\", wires=4)\n",
    "# Random circuit parameters\n",
    "rand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(phi=None):\n",
    "    # Encoding of 4 classical input values\n",
    "    for j in range(4):\n",
    "        angle = float(phi[j] * np.pi)\n",
    "        #print(type(angle))\n",
    "        qml.RY(angle, wires=j)\n",
    "\n",
    "    # Random quantum circuit\n",
    "    RandomLayers(rand_params, wires=list(range(4)))\n",
    "\n",
    "    # Measurement producing 4 classical output values\n",
    "    return [qml.expval(qml.PauliZ(j)) for j in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quanv(image):\n",
    "    \"\"\"Convolves the input image with many applications of the same quantum circuit.\"\"\"\n",
    "    out = np.zeros((14, 14, 4))\n",
    "\n",
    "    # Loop over the coordinates of the top-left pixel of 2X2 squares\n",
    "    for j in range(0, 28, 2):\n",
    "        for k in range(0, 28, 2):\n",
    "            # Process a squared 2x2 region of the image with a quantum circuit\n",
    "            q_results = circuit(\n",
    "                phi=[image[j, k, 0], image[j, k + 1, 0], image[j + 1, k, 0], image[j + 1, k + 1, 0]]\n",
    "            )\n",
    "            # Assign expectation values to different channels of the output pixel (j/2, k/2)\n",
    "            for c in range(4):\n",
    "                out[j // 2, k // 2, c] = q_results[c]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum pre-processing of train images:\n",
      "24/30        \r"
     ]
    }
   ],
   "source": [
    "if PREPROCESS == True:\n",
    "    q_train_images = []\n",
    "    print(\"Quantum pre-processing of train images:\")\n",
    "    for idx, img in enumerate(train_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_train), end=\"\\r\")\n",
    "        q_train_images.append(quanv(img))\n",
    "    q_train_images = np.asarray(q_train_images)\n",
    "\n",
    "    q_test_images = []\n",
    "    print(\"\\nQuantum pre-processing of test images:\")\n",
    "    for idx, img in enumerate(test_images):\n",
    "        print(\"{}/{}        \".format(idx + 1, n_test), end=\"\\r\")\n",
    "        q_test_images.append(quanv(img))\n",
    "    q_test_images = np.asarray(q_test_images)\n",
    "\n",
    "    # Save pre-processed images\n",
    "    np.save(SAVE_PATH + \"q_train_images.npy\", q_train_images)\n",
    "    np.save(SAVE_PATH + \"q_test_images.npy\", q_test_images)\n",
    "\n",
    "\n",
    "# Load pre-processed images\n",
    "q_train_images = np.load(SAVE_PATH + \"q_train_images.npy\")\n",
    "q_test_images = np.load(SAVE_PATH + \"q_test_images.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "n_channels = 4\n",
    "fig, axes = plt.subplots(1 + n_channels, n_samples, figsize=(10, 10))\n",
    "for k in range(n_samples):\n",
    "    axes[0, 0].set_ylabel(\"Input\")\n",
    "    if k != 0:\n",
    "        axes[0, k].yaxis.set_visible(False)\n",
    "    axes[0, k].imshow(train_images[k, :, :, 0], cmap=\"gray\")\n",
    "\n",
    "    # Plot all output channels\n",
    "    for c in range(n_channels):\n",
    "        axes[c + 1, 0].set_ylabel(\"Output [ch. {}]\".format(c))\n",
    "        if k != 0:\n",
    "            axes[c, k].yaxis.set_visible(False)\n",
    "        axes[c + 1, k].imshow(q_train_images[k, :, :, c], cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyModel():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = MyModel()\n",
    "\n",
    "q_history = q_model.fit(\n",
    "    q_train_images,\n",
    "    train_labels,\n",
    "    validation_data=(q_test_images, test_labels),\n",
    "    batch_size=4,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model = MyModel()\n",
    "\n",
    "c_history = c_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size=4,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 9))\n",
    "\n",
    "ax1.plot(q_history.history[\"val_accuracy\"], \"-ob\", label=\"With quantum layer\")\n",
    "ax1.plot(c_history.history[\"val_accuracy\"], \"-og\", label=\"Without quantum layer\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(q_history.history[\"val_loss\"], \"-ob\", label=\"With quantum layer\")\n",
    "ax2.plot(c_history.history[\"val_loss\"], \"-og\", label=\"Without quantum layer\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_ylim(top=2.5)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
